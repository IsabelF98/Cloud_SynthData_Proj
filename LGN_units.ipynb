{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586e23a2-180a-457b-bbed-a7e6d6a8951e",
   "metadata": {},
   "source": [
    "# LGN Unit Analysis\n",
    "\n",
    "Looking at how many LGN units are needed in the model.\n",
    "\n",
    "January 17, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b929fc-7447-4924-b6af-bbeb68a9e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Computer: [sc]\n",
      "Invoking __init__.py for NDNT.utils\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "myhost = os.uname()[1]\n",
    "print(\"Running on Computer: [%s]\" %myhost)\n",
    "\n",
    "sys.path.insert(0, '/home/ifernand/Code/') \n",
    "dirname = '/home/ifernand/Cloud_SynthData_Proj'\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io as sio\n",
    "import torch\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# NDN tools\n",
    "import NDNT\n",
    "import NDNT.utils as utils\n",
    "from NDNT.modules.layers import *\n",
    "from NDNT.networks import *\n",
    "import NDNT.NDN as NDN\n",
    "from NTdatasets.conway.synthcloud_datasets import SimCloudData\n",
    "from NTdatasets.generic import GenericDataset\n",
    "from ColorDataUtils.multidata_utils import MultiExperiment\n",
    "import ColorDataUtils.ConwayUtils as CU\n",
    "from ColorDataUtils import readout_fit\n",
    "from ColorDataUtils.simproj_utils import *\n",
    "from NDNT.utils import fit_lbfgs, fit_lbfgs_batch\n",
    "from NDNT.utils import imagesc   \n",
    "from NDNT.utils import ss\n",
    "from NDNT.utils import subplot_setup\n",
    "from NDNT.utils import figure_export\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device0 = torch.device(\"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dafb67-dce6-43e9-a81e-e80812c07468",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64952758-3e8b-4eec-9b36-ea336ea8b4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time 53.34195899963379 sec\n"
     ]
    }
   ],
   "source": [
    "start    = time.time()\n",
    "data     = SimCloudData(cell_type_list=['V1_Exc_L4', 'V1_Inh_L4', 'V1_Exc_L2/3', 'V1_Inh_L2/3'], down_sample=2, num_lags=11)\n",
    "end      = time.time()\n",
    "print('CPU Time', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45777b7a-e6c7-44d4-a46b-29f651566cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline LL\n",
    "GLM_LL = np.load('data/cloud_data_stim_dim_120_sqrad_0.3_GLM_LL.pkl', allow_pickle=True)\n",
    "GQM_LL = np.load('data/cloud_data_stim_dim_120_sqrad_0.3_GQM_LL.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c8e3bf-dde3-43eb-a9d0-c8ea6aded4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_idx_dict = data.cell_idx_dict\n",
    "thetas_dict = data.thetas\n",
    "V1_thetas = np.concatenate((thetas_dict['V1_Exc_L4'], thetas_dict['V1_Inh_L4'], thetas_dict['V1_Exc_L2/3'], thetas_dict['V1_Inh_L2/3']))\n",
    "mu0s = data.mu0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ee789b-27e1-42f1-84a7-84d727f01387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stim_dims = [1, 60, 60, 1]\n",
      "num_lags = 11\n",
      "L = 60\n",
      "Number of cells 1491\n",
      "Number of time points 510000\n"
     ]
    }
   ],
   "source": [
    "stim_dims = data.stim_dims\n",
    "num_lags = data.num_lags\n",
    "L  = stim_dims[1]\n",
    "NC = data.NC\n",
    "NT = data.NT\n",
    "print('stim_dims =', stim_dims)\n",
    "print('num_lags =', num_lags)\n",
    "print('L =', L)\n",
    "print('Number of cells', NC)\n",
    "print('Number of time points', NT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630eac3-bc1d-43b0-88b3-aa9fdb5a4e59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Degree to Qmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b0de32-e528-4d09-8573-cea7a54cbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrees2mu(theta_deg, angles, continuous=True, max_angle=180 ):\n",
    "    \"\"\"\n",
    "    Converts degrees into mu-values. If to_output=True, outputs to an array, and otherwise\n",
    "    stores in the Qmu variable. It detects whether half-circle of full circle using stored angle values\n",
    "        \n",
    "    Args:\n",
    "        theta_deg (np array): array of angles in degrees into mu values, based on 180 or 360 deg wrap-around\n",
    "        continuous (Boolean): whether to convert to continuous angle or closest \"integer\" mu value (def True, continuous)\n",
    "        max_angle: maximum angle represented in OriConv layers (default 180, but could be 360)\n",
    "    Returns:\n",
    "        Qmus: as numpy-array, if to_output is set to True, otherwise, nothing \n",
    "    \"\"\"\n",
    "    num_angles = len(angles)\n",
    "\n",
    "    # convert inputs to np.array\n",
    "    if not isinstance(theta_deg, np.ndarray):\n",
    "        theta_deg = np.array(theta_deg, dtype=np.float32)\n",
    "    if not continuous:\n",
    "        dQ = max_angle/num_angles\n",
    "        theta_deg = dQ * np.round(theta_deg/dQ)\n",
    "    theta_deg = (theta_deg%max_angle)  # map between 0 and max_angle\n",
    "\n",
    "    mu_offset = 1/num_angles # first bin at 0 degrees is actually a shifted mu value (not right at edge)\n",
    "    Qmus = (theta_deg-max_angle/2) / (max_angle/2) + mu_offset\n",
    "    Qmus[Qmus <= -1] += 2\n",
    "    Qmus[Qmus > 1] += -2\n",
    "    \n",
    "    return Qmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e904abf-78e0-4ba4-90d1-99368e6ac0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle mu0: (1491,)\n"
     ]
    }
   ],
   "source": [
    "angles = np.arange(0, 180, 30).astype(int)\n",
    "Qmu0s = degrees2mu(V1_thetas, angles)\n",
    "print('Angle mu0:', Qmu0s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbfc17-892b-4b6f-ae71-1f3a214bfd98",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8fa3a9-6220-4a23-b5fb-25e22fd918f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Parameters\n",
    "adam_pars = utils.create_optimizer_params(\n",
    "    optimizer_type='AdamW', batch_size=1,\n",
    "    learning_rate=0.01, early_stopping_patience=4,\n",
    "    optimize_graph=False, weight_decay=0.2, accumulated_grad_batches=3)\n",
    "adam_pars['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5680a997-f7ff-4367-bc49-a257e2d415d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 created\n",
      "Model 1 created\n",
      "Model 2 created\n",
      "Model 3 created\n"
     ]
    }
   ],
   "source": [
    "XTreg = 0.0001\n",
    "Xreg0 = 1.0 # d2/dx\n",
    "Creg0 = 0.05 # center\n",
    "\n",
    "Xreg1 = 0.05 # d2/dx\n",
    "Creg1 = 0.001 # center\n",
    "\n",
    "MaxReg = 0.001\n",
    "\n",
    "angle_mode = 'nearest' # 'bilinear'\n",
    "\n",
    "NQ = len(angles)\n",
    "\n",
    "num_LGN_units = [1,2,3,4]\n",
    "models = {}\n",
    "\n",
    "for i in range(len(num_LGN_units)):\n",
    "    num_subs = [num_LGN_units[i], 24, 16, 16]\n",
    "    fws = [19, 19, 5, 5]  \n",
    "\n",
    "    # LGN LAYER\n",
    "    clayersQ = [STconvLayer.layer_dict( \n",
    "        input_dims = data.stim_dims, num_filters=num_subs[0], norm_type=1,\n",
    "        filter_dims=[1,fws[0],fws[0],num_lags-1] , bias=False, NLtype='relu',\n",
    "        padding='circular', output_norm='batch', window='hamming', initialize_center=True,\n",
    "        reg_vals={'d2xt':XTreg, 'd2x':Xreg0, 'center': Creg0} )]\n",
    "\n",
    "    # PROJECTION LAYER\n",
    "    clayersQ.append(\n",
    "        OriConvLayer.layer_dict(\n",
    "            num_filters=num_subs[1], num_inh=num_subs[1]//2,\n",
    "            filter_width=fws[1], NLtype='relu', norm_type=1,\n",
    "            bias=False, output_norm='batch', window='hamming', padding='circular', initialize_center=True, \n",
    "            reg_vals={'d2x':Xreg1, 'center': Creg1}, angles=angles) )\n",
    "\n",
    "    # TIME SHIFT LAYER\n",
    "    clayersQ.append(TimeShiftLayer.layer_dict())\n",
    "\n",
    "    # REST\n",
    "    for ii in range(2,len(fws)):\n",
    "        clayersQ.append(OriConvLayer.layer_dict( \n",
    "            num_filters=num_subs[ii], num_inh=num_subs[ii]//2, bias=False, norm_type=1, \n",
    "            filter_width=fws[ii], NLtype='relu',\n",
    "            output_norm='batch', initialize_center=True, #window='hamming', \n",
    "            angles=angles) )\n",
    "        \n",
    "    scaffold_netQ =  FFnetwork.ffnet_dict(\n",
    "        ffnet_type='scaffold3d', xstim_n='stim', layer_list=clayersQ, scaffold_levels=[1,3,4], num_lags_out=NQ)\n",
    "\n",
    "    readout_parsQ = ReadoutLayerQsample.layer_dict(\n",
    "        num_filters=NC, NLtype='softplus', bias=True, pos_constraint=True,\n",
    "        reg_vals={'max': MaxReg})\n",
    "\n",
    "    readout_netQ = FFnetwork.ffnet_dict(xstim_n = None, ffnet_n=[0], layer_list = [readout_parsQ], ffnet_type='readout')\n",
    "    \n",
    "    models[str(num_LGN_units[i])+' LGN Units'] = NDN(ffnet_list = [scaffold_netQ, readout_netQ], loss_type='poisson', seed=100)\n",
    "    print('Model', i, 'created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d19f4-fb44-480f-a330-7182edd54ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ReadoutLayer: fitting mus\n",
      "  ReadoutLayer: not fitting Qmus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval models: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:33<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 done fitting\n",
      "  ReadoutLayer: fitting mus\n",
      "  ReadoutLayer: not fitting Qmus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval models: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:37<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 done fitting\n",
      "  ReadoutLayer: fitting mus\n",
      "  ReadoutLayer: not fitting Qmus\n"
     ]
    }
   ],
   "source": [
    "LL_dict = {}\n",
    "for i in range(len(num_LGN_units)):\n",
    "    cnn = models[str(num_LGN_units[i])+' LGN Units']\n",
    "    cnn.networks[1].layers[0].mu.data = torch.tensor(mu0s, dtype=torch.float32)\n",
    "    cnn.networks[1].layers[0].Qmu.data[:,0] = torch.tensor(Qmu0s, dtype=torch.float32)\n",
    "    cnn.networks[1].layers[0].fit_mus(True)\n",
    "    cnn.networks[1].layers[0].fit_Qmus(False)\n",
    "    cnn.networks[1].layers[0].Qsample_mode = angle_mode\n",
    "    cnn.block_sample = True\n",
    "\n",
    "    start = time.time()\n",
    "    cnn.fit(data, **adam_pars, verbose=False)\n",
    "    end = time.time()\n",
    "\n",
    "    LL_dict[str(num_LGN_units[i])+' LGN Units'] = cnn.eval_models(data, data_inds=data.val_blks, device=device, batch_size=3, null_adjusted=True)\n",
    "\n",
    "    print('Model', i, 'done fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad65d36-2958-4bfb-a7f6-7e4c4ac1a5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
